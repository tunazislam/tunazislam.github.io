---
title: "Can LLMs Assist Annotators in Identifying Morality Frames? - Case Study on Vaccination Debate on Social Media"
collection: publications
permalink: /publications/llms-annotator-morality
venue: "17th ACM Web Science Conference (WebSci 2025)"
citation: '<b>Tunazzina Islam</b>, Dan Goldwasser. 17th ACM Web Science Conference (WebSci 2025)</a>.'

---
[[arXiv]](https://arxiv.org/pdf/2502.01991) 

## Abstract
Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions. In NLP, data scarcity and complexity of psycholinguistic tasks, such as identifying morality frames, make relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load. To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles. Our research explores LLMs' potential to assist human annotators in identifying morality frames within vaccination debates on social media. We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a "think-aloud" tool. Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks.
